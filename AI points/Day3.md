## 1.   什么是集成学习算法

集成学习算法本身不算是一种单独的机器学习算法，而是通过**构建并结合多个机器学习器**来完成学习任务。多个学习器被训练来解决同一个问题。

**与普通机器学习方法的不同点：**
    普通机器学习试图从训练数据中学习一个假设。
    集成方法试图构建一组假设并将它们组合起来。

**集成学习方法的优缺点：**
    优点：能在机器学习算法中拥有较高的**准确率**。
    缺点：模型的训练过程可能会比较**复杂**，效率不高


## 2.   集成学习主要有哪几种框架，简述工作过程

**常见的集成学习算法有2种：**
    1.  串行集成方法：通过基础模型之间的**依赖**，给错误分类样本一个较大的权重来提升模型的性能。

    2.  并行集成方法：利用基础模型的**独立性**，通过平均，能够较大的降低误差。

**集成学习的四种学习框架：**

1.  基于Bagging的算法：对样本训练集合进行随机化抽样，通过反复的抽样训练新模型，最终在这些模型的基础上取平均。
   
    基本思想：
        1.  给定一个弱学习算法，一个训练集合。
        2.  单个弱学习算法的准确率不高。
        3.  将此弱学习算法使用多次，得出预测函数序列，进行投票。
        4.  准确率得到提高。

    代表算法：随机森林

2.  基于Boosting的算法：一种迭代算法。将弱学习器提升为强学习器的算法。通过不断的使用一个弱学习器来弥补前一个弱学习器的“不足”的过程，串行的构造一个较强的学习器。

    基本思想：
        1.  先赋予每个训练样本相同的概率。
        2.  进行 T 次迭代，每次迭代后，对分类错误的样本加大权重，是的在下一次的迭代中更加关注这些样本。

    代表算法：Adaboost算法

3.  基于Stacking的算法

4.  基于Blending的算法

**Bagging 和 Boosting的区别**

1.  样本选择：
    
    Bagging：训练机是在原始集中有放回选取的，从原始集中选取的各轮训练集之间是独立的。
    Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化，而权重是根据上一轮的分类结果进行调整的。

2.  样例权重：

    Bagging：均匀取样，每个样例的权重相等。
    Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。

3.  预测函数：

    Bagging：所有预测函数的权重相等。
    Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。

4.  并行计算：

    Bagging：预测函数可以并行生成。
    Boosting：预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。

## 3.   Boosting算法有哪两类，区别

1.  AdaBoost算法：  通过训练数据的分布构造一个分类器，然后通过误差率求出这个弱分类器的权重，通过更新训练数据的分布，迭代进行，直到达到迭代次数或是损失函数小于某一筏值。

2.  GDBT算法：GDBT为了减少上一次的残差，在残差减少的方向上建立一个新的模型。

## 4.   什么是偏差和方差

方差：一个随机变量的方差描述的是他的离散程度，就是该变量离其期望值的距离。
偏差：值得是个别测定值与测定的平均值之差。用来衡量测定结果的精密度高低。

## 5.   为什么说Bagging可以减少弱分类器的方差，而Boosting可以减少弱分类器的偏差

Bagging：均匀取样，每个样例的权重相等。

Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。

## 6.   简述一下随机森林算法的原理

随机森林 = Bagging + 决策树

随机森林算法：从原始训练样本集 N 中有放回的重复随机抽取 k 个样本生成新的训练样本集合，根据自主样本集生成 k 个分类树组成随机森林。新数据的分类结果按分类树投票多少形成的分数而定。

实质上是对决策树算法的一种改进，将多个决策树合并在一起，每棵树的建立依赖于一个独立抽取的样品，森林中的每棵树具有相同的分布，分类误差取决于每一棵树的分类能力和他们之间的相关性。

## 7.   随机森林的随机性体现在哪里

特征选择。

随机森林采用随机的方法去分裂每一个节点，然后比较不用情况下产生的误差。
单棵树的分类能力可能很小，但是在随机产生大量的决策树之后，一个测试样品可以用过每一棵树的分类结果统计后选择最可能的分类。

