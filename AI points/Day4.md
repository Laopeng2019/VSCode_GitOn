## 1.  简述 GBDT 原理 

GBDT (Gradient Boosting Decision Tree)：梯度提升树。

Decision Tree：CART 回归树。

    GBDT 使用的决策树是 CART 回归树，处理回归问题还是二分类问题以及多分类问题，GBDT 使用的决策树都是 CART 回归树。

    为什么不用 CART 分类树：因为 GBDT 每次迭代都要拟合的是 梯度值，是连续值所以要用回归树。

    对于回归树算法来说最重要的就是寻找最佳的划分点，回归树中的可划分点包含了所有特征的所有可取的值。
    在分类树中最佳划分点的判别标准是熵or基尼系数，都是用纯度来衡量的。
    回归树中的样本标签是连续数值，所以在回归树中指标选用**平方误差**，能够很好的评判拟合程度。

Gradient Boosting：拟合负梯度

    例子：
    有个人30岁
    第一轮：用20岁去拟合，发现损失有10岁。
    第二轮：用6岁去拟合剩下的损失，发现还有差距4岁。
    第三轮：用3岁去拟合，差距只有1岁了。

    每一轮迭代，拟合的岁数误差都会减小。
    模型的输出结果：每次拟合的岁数相加 20+6+3 = 29。

    每一次迭代的残差 就是 与30岁的差距。

GBDT = Decision Tree + Gradient Boosting

    1.  初始化弱学习器
    2.  计算残差 --> 将残差作为样本的新的真实值 --> 计算最佳拟合值 --> 更新强学习器
    3.  得到最终强学习器

[具体的例子](https://ranmaosong.github.io/2019/04/27/ML-GBDT/)


## 2.   GBDT常用损失函数

1.  均方差
2.  绝对损失
3.  Huber损失
4.  分位数损失

噪音点不多 选默认的均方差；噪音点较多，选抗噪音的损失函数 Huber；需要对训练集进行分段预测， 选分位数损失。

GBDT 应用于回归问题时：回归问题的损失函数一般为 平方差损失函数。
这时的残差，恰好等于预测值与实际值之间的差值。每次拿一棵决策树去拟合这个差值，使得残差越来越小


## 3.   GBDT 如何用于分类

用一系列的梯度提升树，去拟合对数几率，最终得到一系列的 CART 回归树。

(https://zhuanlan.zhihu.com/p/46445201)


## 4.   为什么 GBDT 不适用于使用高维稀疏特征

1.  会产生样本切分不平衡的问题。

2.  影响决策树的学习。


## 5.   GBDT 算法的优缺点

优点：
    1.  可以灵活处理各种类型的数据：连续/离散。
    2.  很好的利用了弱分类器进行级联。
    3.  充分考虑了每个分类器的权重。

缺点：
    1.  弱学习器之间存在依赖关系，难以并行训练数据。


## 6.   简述 XGBoost

核心算法思想

1.  不断地添加树，不断地进行特征分裂来生长一棵树，每次添加一个树，其实是学习一个新函数f(x)，去拟合上次预测的残差。

2.  当我们训练完成得到k棵树，我们要预测一个样本的分数，其实就是根据这个样本的特征，在每棵树中会落到对应的一个叶子节点，每个叶子节点就对应一个分数。

3.  最后只需要将每棵树对应的分数加起来就是该样本的预测值。

## 7.   XGBoost 和 GBDT 区别

1.  传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。顺便提一下，xgboost工具支持自定义代价函数，只要函数可一阶和二阶求导。

2.  xgboost在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。从Bias-variance tradeoff角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是xgboost优于传统GBDT的一个特性。

3.  xgboost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。实际应用中，一般把eta设置得小一点，然后迭代次数设置得大一点。（补充：传统GBDT的实现也有学习速率）

4.  xgboost借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算。